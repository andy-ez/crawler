= Web Crawler

Simple command line run web crawler that visits every reachable page under a domain and outputs the URLs of every static asset, grouped by page url.


== Installation

* Download the project folder then in a terminal session navigate to the project's directory and run:

    bundle install

  Note this application will require a working Ruby installation.


== Instructions

* Application can be run from the command line passing in a valid url as an argument to the crawl command, e.g:

    ./crawler crawl http://example.com

* As a default a limit of 250 is set on the total number of visited urls. This parameter can be changed by passing in a number as the second argument to the crawl command, e.g:

    ./crawler crawl http://example.com 400